services:
  postgres:
    image: postgres:15-alpine
    container_name: n8n_postgres
    restart: always
    env_file:
      - .env
    volumes:
      - ./instance/postgres_data:/var/lib/postgresql/data
      - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
      interval: 5s
      timeout: 5s
      retries: 10

  n8n:
    build:
      context: ./docker
      dockerfile: Dockerfile
    image: n8n_custom:latest
    container_name: n8n_server
    user: "node"
    ports:
      - "5678:5678"
    env_file:
      - .env
    volumes:
      - ./instance/n8n_data:/home/node/.n8n
      - ./workflows:/home/node/.n8n/workflows
      - ./custom/dist:/home/node/.n8n/custom
      - ./docker/uploads:/home/node/uploads
    working_dir: /home/node/.n8n
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/rest/healthz"]
      interval: 30s
      timeout: 5s
      retries: 3
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"

  ollama:
    image: ollama/ollama:0.12.11
    container_name: n8n_ollama
    ports:
      - "11434:11434"
    volumes:
      - ./instance/ollama/ollama/id_ed25519:/root/.ollama/id_ed25519
      - ./instance/ollama/ollama/id_ed25519.pub:/root/.ollama/id_ed25519.pub
      - ./instance/ollama/models:/root/.ollama/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  ollama_fallback:
    image: ollama/ollama:0.12.11
    container_name: n8n_ollama_fallback
    ports:
      - "11435:11434"
    volumes:
      - ./instance/ollama/ollama_fallback/id_ed25519:/root/.ollama/id_ed25519
      - ./instance/ollama/ollama_fallback/id_ed25519.pub:/root/.ollama/id_ed25519.pub
      - ./instance/ollama/models:/root/.ollama/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  postgres_data:
  n8n_data:
  ollama:
  ollama_fallback:
